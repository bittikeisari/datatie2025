{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2f41b193-221d-408d-bd70-18eba1cd6860",
   "metadata": {},
   "source": [
    "## Valmistautuminen\n",
    "\n",
    "Jos käytössäsi on Windows-kone, saatat joutua tekemään lisäasennuksia jotta saat [fastText-kirjaston asennettua](https://medium.com/@oleg.tarasov/building-fasttext-python-wrapper-from-source-under-windows-68e693a68cbb). \n",
    "Toinen vaihtoehto on toteuttaa tämä harjoitus [Google Colabissa](https://colab.research.google.com/). \n",
    "\n",
    "\n",
    "1. Asennettava: <code>fasttext</code>, <code>nltk</code>, <code>spacy</code> \n",
    "1. Käynnistä kernel uudelleen\n",
    "1. Jos virheet jatkuvat, asenna:  <code>spacy-transformers</code> \n",
    "1. Käynnistä kernel uudelleen\n",
    "1. <code>python -m spacy download fi_core_news_sm</code>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e6adce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install fasttext nltk spacy\n",
    "#!pip install spacy-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "88d810e0-ad62-4ad2-8cc2-c59e8d81bcbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting fi-core-news-sm==3.8.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/fi_core_news_sm-3.8.0/fi_core_news_sm-3.8.0-py3-none-any.whl (14.3 MB)\n",
      "     ---------------------------------------- 0.0/14.3 MB ? eta -:--:--\n",
      "     - -------------------------------------- 0.5/14.3 MB 3.4 MB/s eta 0:00:05\n",
      "     -- ------------------------------------- 1.0/14.3 MB 2.8 MB/s eta 0:00:05\n",
      "     --- ------------------------------------ 1.3/14.3 MB 2.1 MB/s eta 0:00:07\n",
      "     ---- ----------------------------------- 1.6/14.3 MB 1.9 MB/s eta 0:00:07\n",
      "     ----- ---------------------------------- 2.1/14.3 MB 2.0 MB/s eta 0:00:07\n",
      "     ------ --------------------------------- 2.4/14.3 MB 2.1 MB/s eta 0:00:06\n",
      "     ------- -------------------------------- 2.6/14.3 MB 1.8 MB/s eta 0:00:07\n",
      "     -------- ------------------------------- 3.1/14.3 MB 1.9 MB/s eta 0:00:06\n",
      "     ---------- ----------------------------- 3.9/14.3 MB 2.1 MB/s eta 0:00:06\n",
      "     ------------ --------------------------- 4.5/14.3 MB 2.1 MB/s eta 0:00:05\n",
      "     ------------- -------------------------- 5.0/14.3 MB 2.2 MB/s eta 0:00:05\n",
      "     ---------------- ----------------------- 5.8/14.3 MB 2.3 MB/s eta 0:00:04\n",
      "     ----------------- ---------------------- 6.3/14.3 MB 2.3 MB/s eta 0:00:04\n",
      "     ------------------- -------------------- 6.8/14.3 MB 2.3 MB/s eta 0:00:04\n",
      "     -------------------- ------------------- 7.3/14.3 MB 2.3 MB/s eta 0:00:04\n",
      "     --------------------- ------------------ 7.9/14.3 MB 2.4 MB/s eta 0:00:03\n",
      "     ---------------------- ----------------- 8.1/14.3 MB 2.3 MB/s eta 0:00:03\n",
      "     ------------------------ --------------- 8.7/14.3 MB 2.3 MB/s eta 0:00:03\n",
      "     ------------------------- -------------- 9.2/14.3 MB 2.3 MB/s eta 0:00:03\n",
      "     --------------------------- ------------ 9.7/14.3 MB 2.3 MB/s eta 0:00:03\n",
      "     ---------------------------- ----------- 10.2/14.3 MB 2.3 MB/s eta 0:00:02\n",
      "     ----------------------------- ---------- 10.7/14.3 MB 2.3 MB/s eta 0:00:02\n",
      "     ------------------------------- -------- 11.3/14.3 MB 2.3 MB/s eta 0:00:02\n",
      "     --------------------------------- ------ 12.1/14.3 MB 2.4 MB/s eta 0:00:01\n",
      "     ----------------------------------- ---- 12.6/14.3 MB 2.4 MB/s eta 0:00:01\n",
      "     ------------------------------------ --- 13.1/14.3 MB 2.4 MB/s eta 0:00:01\n",
      "     -------------------------------------- - 13.9/14.3 MB 2.4 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 14.3/14.3 MB 2.4 MB/s eta 0:00:00\n",
      "Installing collected packages: fi-core-news-sm\n",
      "Successfully installed fi-core-news-sm-3.8.0\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('fi_core_news_sm')\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download fi_core_news_sm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8f831c4-ac70-4ec8-a085-63351ab20c38",
   "metadata": {},
   "source": [
    "Following Teemu's presentation earlier this week, the trained FastText classifier was downloaded using the following lines:\n",
    "<code>\n",
    "    ft_classifier.quantize(input=input, retrain=True)\n",
    "    ft_classifier.save_model(\"ft_classifier.ftz\")\n",
    "</code>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "06603593-58ab-44ca-888b-c39570ff97c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import fasttext\n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "# import libvoikko #perusmutoistamiseen käytetty kirjasto\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fcc6bf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python -m spacy download fi_core_news_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "489ff581-b45d-4791-8466-2286768a00ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Valtteri\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp = spacy.load(\"fi_core_news_sm\")\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c5b45ab1-2f40-4e9f-bf55-5289855b371f",
   "metadata": {},
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mstopwords\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('stopwords')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/stopwords\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\Valtteri/nltk_data'\n    - 'c:\\\\Users\\\\Valtteri\\\\anaconda3\\\\nltk_data'\n    - 'c:\\\\Users\\\\Valtteri\\\\anaconda3\\\\share\\\\nltk_data'\n    - 'c:\\\\Users\\\\Valtteri\\\\anaconda3\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\Valtteri\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\Valtteri\\anaconda3\\Lib\\site-packages\\nltk\\corpus\\util.py:84\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     83\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 84\u001b[0m     root \u001b[38;5;241m=\u001b[39m nltk\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msubdir\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mzip_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     85\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\Valtteri\\anaconda3\\Lib\\site-packages\\nltk\\data.py:579\u001b[0m, in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    578\u001b[0m resource_not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 579\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[1;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mstopwords\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('stopwords')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/stopwords.zip/stopwords/\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\Valtteri/nltk_data'\n    - 'c:\\\\Users\\\\Valtteri\\\\anaconda3\\\\nltk_data'\n    - 'c:\\\\Users\\\\Valtteri\\\\anaconda3\\\\share\\\\nltk_data'\n    - 'c:\\\\Users\\\\Valtteri\\\\anaconda3\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\Valtteri\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(stopwords\u001b[38;5;241m.\u001b[39mwords(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfinnish\u001b[39m\u001b[38;5;124m'\u001b[39m))\n",
      "File \u001b[1;32mc:\\Users\\Valtteri\\anaconda3\\Lib\\site-packages\\nltk\\corpus\\util.py:120\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__getattr__\u001b[1;34m(self, attr)\u001b[0m\n\u001b[0;32m    117\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m attr \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__bases__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    118\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLazyCorpusLoader object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__bases__\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 120\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__load()\n\u001b[0;32m    121\u001b[0m \u001b[38;5;66;03m# This looks circular, but its not, since __load() changes our\u001b[39;00m\n\u001b[0;32m    122\u001b[0m \u001b[38;5;66;03m# __class__ to something new:\u001b[39;00m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, attr)\n",
      "File \u001b[1;32mc:\\Users\\Valtteri\\anaconda3\\Lib\\site-packages\\nltk\\corpus\\util.py:86\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     84\u001b[0m             root \u001b[38;5;241m=\u001b[39m nltk\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msubdir\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mzip_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     85\u001b[0m         \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m:\n\u001b[1;32m---> 86\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m     88\u001b[0m \u001b[38;5;66;03m# Load the corpus.\u001b[39;00m\n\u001b[0;32m     89\u001b[0m corpus \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__reader_cls(root, \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__args, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__kwargs)\n",
      "File \u001b[1;32mc:\\Users\\Valtteri\\anaconda3\\Lib\\site-packages\\nltk\\corpus\\util.py:81\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     79\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     80\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 81\u001b[0m         root \u001b[38;5;241m=\u001b[39m nltk\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msubdir\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     82\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     83\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\Valtteri\\anaconda3\\Lib\\site-packages\\nltk\\data.py:579\u001b[0m, in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    577\u001b[0m sep \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m70\u001b[39m\n\u001b[0;32m    578\u001b[0m resource_not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 579\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[1;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mstopwords\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('stopwords')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/stopwords\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\Valtteri/nltk_data'\n    - 'c:\\\\Users\\\\Valtteri\\\\anaconda3\\\\nltk_data'\n    - 'c:\\\\Users\\\\Valtteri\\\\anaconda3\\\\share\\\\nltk_data'\n    - 'c:\\\\Users\\\\Valtteri\\\\anaconda3\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\Valtteri\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "print(stopwords.words('finnish'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b13488d-7a28-4edb-b397-3e5bcb8fd483",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load tweets datasets\n",
    "# Note: May need to install openpyxl library for the following line to work\n",
    "# pip install openpyxl\n",
    "df = pd.read_excel('shared_data/sample_texts.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d73a0de-24a5-42b8-b06a-20b3632a18ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "077ddcd3-76bd-4f86-adf8-0b077096b388",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = df[['text_nousernames', 'eduskunta-vkk']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69c0b52e-8ea4-4e8a-a8f7-2a6f6988419b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f30fc1dc-ae26-43f9-b614-907c612e56e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Methods imported from the original Notebook\n",
    "def preprocess_word(word, stopwords):\n",
    "    word = re.sub(\"[^A-Za-z0-9ÄäÖö-]+\", \"\", word)\n",
    "    word = re.sub(\"-\", \" \", word)\n",
    "    word = word.strip().lower()\n",
    "    if len(word) < 2:\n",
    "        return \"\"\n",
    "    if type(word) != None:\n",
    "        if word in stopwords:\n",
    "            return \"\"\n",
    "        return word\n",
    "\n",
    "def preprocess_sent(sent, stopwords):\n",
    "    if sent != \"\":\n",
    "        # lemmatize words using spacy\n",
    "        doc = nlp(sent)\n",
    "        sent = \" \".join([token.lemma_ for token in doc])\n",
    "        \n",
    "        words = sent.strip().split(\" \")\n",
    "        final_sent = \" \".join([preprocess_word(word, stopwords) for word in words if type(word) != None])\n",
    "        return final_sent\n",
    "\n",
    "def preprocess_row(teksti, stopwords):\n",
    "    if type(teksti) == str:\n",
    "        sents = teksti.strip().split(\".\")\n",
    "        sents = filter(None,sents)\n",
    "        teksti = \".\".join([preprocess_sent(sent, stopwords) for sent in sents if type(sent) != None])\n",
    "        return teksti"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cab51486-0917-476d-8969-4d52f8925766",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4083051a-bc7e-4822-b98b-28a26bfda7e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4037b15-b46e-4e3c-8047-166210a8b785",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30405310-3c88-44c6-92e9-d6b6f305cfcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Apufunktio, jolla voidaan luoda fastText-yhteensopiva opetus- ja tekstitiedosto\n",
    "\n",
    "def preprocess_ft_df(df):\n",
    "    df[\"label\"] = [\"__label__\" + \"_\".join(str(x).split(\" \")) for x in df[\"eduskunta-vkk\"].values]\n",
    "    df[\"preprocessed\"] = [preprocess_row(sentence, stopwords.words('finnish')) for sentence in df[\"text_nousernames\"]]\n",
    "    df = df.drop([\"eduskunta-vkk\", \"text_nousernames\"], axis = 1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7afa6ce6-bdea-47a4-ac44-17754362c9c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = preprocess_ft_df(tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cd3ae38-f572-4c41-9b8b-b9af934e6e44",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a6c9652-1de9-4afb-b928-69e25e90e403",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ca91d24-b379-411d-86c1-254b5b62ba4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = fasttext.load_model(\"model_filename.ftz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "054073ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.predict('Hiilen sidonta vaatii toimenpiteitä metsäpolitiikassa')[0][0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d675f54-6adc-4215-a3f2-a5f7956506fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Produce predictions for all tweets. Measure the overall accuracy (correct predictions / mistakes)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e7afaa7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6c077db-3b0b-4837-a674-d9392f6ab6b3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a1d1ee2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f253361",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
